---
title: "Modelling in R"
format: html
theme: sandstone
author-title: Made for
author: Bush Heritage
toc: true
toc-location: left
toc-title: Content
toc-depth: 4
published-title: Date
date: 2024-02-13
editor: visual
embed-resources: true
---

# Statistical modelling in R for ecologists

Whenever you embark on a journey in statistical modelling, it's good to be reminded of the following quote from *George E.P. Box*,

**All models are wrong, but some are useful**.

![Photograph of Geoge E.P. Box, "one of the great statistical minds of the 20th century"](Images/Box.jpeg)

We build models to help us understand ecological relationships. But no model is perfect. Ecological data in particular is fraught with a lot of random, natural variation in observations that our models will be unable to explain. 

Modelling goals can be three-fold: 

  1. **Exploration**, e.g. which environmental variables are associated with species richness?,
  
  2. **Inference**, e.g., do protected areas cause increased species richness?,
  
  3. **Prediction**, e.g., use correlations between environmental variables and species abundance to predict species habitat suitability across space.

How we build and select models depends on our goals for the analysis. Check out this great guide to [goal driven model selection in ecology](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecy.3336)

Here, we will focus primarily on building exploratory and predictive models. Causal inference is beyond the scope, but see this great guide for ecologists [insert Arif paper]. We'll start with a basic linear model to explore how disturbance influences ecological characteristics across survey sites. We'll see how, depending on the type of data we are modelling, we might need use *generalised* linear models (as opposed to *general* linear models). We'll end with building complex spatial and multivariate models in **R**.

This course assumes an understanding of how to wrangle, summarise, and visualise data in **R**. We hope you will learn:

  - The fundamentals of fitting statistical models to data,
  
  - how to check your model fits the data well, and
  
  - how to understand and visualise model output in **R**.

## General linear models

We can use linear models to estimate the strength and direction of ecological relationships. The word 'linear' is important - when we use linear models we assume a straight (or linear) relationship between our response variable (also referred to as dependent variables) and our explanatory variables (also referred to as covariates or predictor variables - don't let the terms confuse you).

To understand how they work, it helps to take a quick look at how we formulate linear models mathematically:

 $$y_i = b_0 + b_{1}X_{1i} + \epsilon_i$$
`y_i` is our response variable of interest,
`b_0` is the is the intercept for the linear relationship between y and our explanatory variables (X's), 
`b_1` is the coefficient representing the strength and direction of the relationship between y and X_1
`\epsilon` is the random, unexplained error in our observations around the fitted relationship between y and X(s).

In the above example we only have one explanatory variable (X), but we can have multiple. We would just add on another term, e.g. `b_{2}X_{2i}`. Explanatory variables can be categorical or continuous - we'll see the consequences of that later.

When we're fitting **general** linear models, we assume that `\epsilon`, the error term, is drawn from a normal (i.e., gaussian) distribution. We would expect this if our response variable (y) is continuous and unbounded (could potentially be any number between $-∞$ and $∞$). In the next section, we'll learn how to fit **generalised** linear models which allow us to model non-normal error distributions necessary for count data, for example.

### Fitting a linear model to data

Let's get started with a general linear model. We want to know, are trees in less disturbed sites on average older compared to disturbed sites.

If you took a first year undergrad stats course, you might be thinking, let's use an ANOVA! ANOVA's are just linear models with a categorical predictor variable.

Tree circumference, our response variable (y) of interest, is continuous and we would expect the error distribution to be normally distributed. So we will fit a general linear model.

Before we fit a model to our data, we should always plot it.
```{r}
dat <- data.frame(circum = c(rnorm(100, 10), rnorm(100, 20)),
                  site = factor(c(rep('Disturbed', 100), c(rep('Undisturbed', 100)))))
boxplot(dat$site, dat$circum)
```

Now let's fit the linear model, `lm`.
```{r}
m <- lm(circum ~ site, data = dat)
summary(m)
```

Let's unpack this model summary. [Talk through model output, if time write notes to explain..]

#### Checking the model fit

Without doing anything else, we already have one metric for assessing model fit - the R^2. It tells us our model has explained a whopping 96% of the variability in the data (note, use the adjusted R^2 if you have more than one explanatory variable). That's alot - the model is doing well! But remember this is not real data (we simulated it) - in ecology we usually see much lower R^2 values due to the natural sources of variability that we can't explain.

In addition to how much variability in the data our model is explaining, we also want to want to make sure the model isn't violating any structural assumptions, namely:

1. Errors are normally distributed,
2. Homogeneity of variance.

We can use model diagnostic plots to check these assumptions:

```{r}
plot(m)
```

Talk through diagnostics [write notes if time...]

#### Bonus: Adding error

What might real data with random, natural variation in sampling look like? Let's add some in and try the model again.

```{r}
dat$circum <- dat$circum + rnorm(nrow(dat), 0, 5)
boxplot(dat$site, dat$circum)
```

```{r}
m2 <- lm(circum ~ site, data = dat)
summary(m2)
```
```{r}
plot(m)
```
How are the coefficients of the model (the intercept and the beta coefficient (b_1) estimated? For a simple, general linear model like this, we (meaning R) uses Ordinarly Least Squares (OLS) to estimate the model coefficients. If you're a visual learner like me, check out [this great tool to see how OLS works](https://seeing-theory.brown.edu/regression-analysis/index.html). In a nutshell, we're iteratively rotating the straight line between our y and x variables until we get the best fit (i.e. have minimised the sums of squared error).

### Generalised linear models

What if, in addition to tree age, we also wanted to ask whether species abundance is higher in disturbed compared to undisturbed sites. Could we use the same model and just swap out circumference for abundance?

Let's think about one of our model assumptions - `\epsilon`, the error term, is normally distributed. Continuous data, like circumference, conforms to this assumption. Observation of species' abundance, however, can only be whole numbers (i.e., is discrete instead of continuous) and is bounded from 0 to $∞$.

So, we need to *generalise* our linear model a bit to accommodate this non-normal error structure. We can use a **link** function to transform our model predictions. [TODO explain this better]

**R** makes this easy to do, we just need to tell is what error distribution (i.e., family) and link function to use.

First let's simulate some abundance data at out survey sites.
```{r}
dat$abundance <- c(rpois(100, 10), rpois(100, 20))
boxplot(dat$site, dat$abundance)
```

Now fit the model

```{r}
mspp <- glm(abundance ~ site, data = dat, family = 'poisson')
summary(mspp)
```

Okay, so what's happened here? It has estimated mean species abundance at the Disturbed site as 2.27 and 0.67 at the Undisturbed site? It's provided the coefficients in the 'link scale' rather than on the 'natural scale'. Here, we've used a poisson distribution, for which the canonical link function is the 'log'. So these are estimates of logged species abundance, instead of raw species abundance. The inverse of the natural log is the exponent. Let's back transform and see what we get what we expect (mean spp abundance of 10 in disturbed and 20 in undistrubed. 

```{r}
exp(2.27213) # intercept (i.e. mean species abundance in disturbed)
exp(0.67965) # beta coefficient
```

So the intercept makes sense, but what about that beta coefficient? Because we are in log space, the relationship is multiplicative rather than additive. 

```{r}
exp(2.27213)*exp(0.67965) # multiply the intercept by the X coefficient rather than sum
```

Voila. Just what we expected.

What else is unusual about this model summary? Now we have null and residual deviance instead of R^2? How come? To estimate coefficients for **generalised linear models** we can no longer use OLS. Instead we use **maximum likelihood**. Explain deviance...deviance residuals, etc. Also discuss dispersion parameter - negative binomial models...

- Explain OLS to max likelihoood
- Plotting and reporting model output

## Day 1 (Afternoon): Complex multivariate models

- Bayesian glm (univariate) - start with same species richness example from previous day,
- Bayesian parameter estimation
- max likelihood to MCMC sampling
- Why Bayesian? complex models (e.g., spatial random effects), low sample size and prior information

- multivariate GLM (or wait till day 2 and make it a spatial example?)



